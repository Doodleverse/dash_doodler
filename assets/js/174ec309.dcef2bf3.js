(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{117:function(e,t,a){"use strict";a.d(t,"a",(function(){return u})),a.d(t,"b",(function(){return h}));var i=a(0),r=a.n(i);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,i,r=function(e,t){if(null==e)return{};var a,i,r={},n=Object.keys(e);for(i=0;i<n.length;i++)a=n[i],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)a=n[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var d=r.a.createContext({}),c=function(e){var t=r.a.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=c(e.components);return r.a.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var a=e.components,i=e.mdxType,n=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(a),b=i,h=u["".concat(s,".").concat(b)]||u[b]||p[b]||n;return a?r.a.createElement(h,o(o({ref:t},d),{},{components:a})):r.a.createElement(h,o({ref:t},d))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var n=a.length,s=new Array(n);s[0]=b;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:i,s[1]=o;for(var d=2;d<n;d++)s[d]=a[d];return r.a.createElement.apply(null,s)}return r.a.createElement.apply(null,a)}b.displayName="MDXCreateElement"},178:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/paperfig_RFchain_ann-b56a8371bc87b77b219a9e0ca2226667.jpg"},179:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_doodles_labelgen-e548fceb28a327b50a42c49e1d3af736.png"},180:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_filt_labelgen-505bc32ee695274f105461ba311a8235.png"},181:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_feats_labelgen-1fdb12ad2e7666a13412a34463fa5fc9.png"},182:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_RFdecsurf_labelgen_ann-c6ae8f7b89f946437b6f998740e9a2b7.png"},183:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_rf_label_filtered_labelgen-4a390b5e15fdb1eda7e8e7070960f6bc.png"},184:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_rf_spatfilt_dist_labelgen_ann-d343c8e126bbdc7114ddbf63d199f0c0.png"},185:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_crf_tta_labelgen-4cb80119f53fcfac89105351cbfadbdd.png"},186:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_crf_label_filtered_labelgen-ee93ef47d939db64179db2323637f279.png"},187:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_label_final_labelgen-9a4b346d54d7a617c37c1e993fd94dff.png"},73:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return s})),a.d(t,"metadata",(function(){return o})),a.d(t,"toc",(function(){return l})),a.d(t,"default",(function(){return c}));var i=a(3),r=a(7),n=(a(0),a(117)),s={sidebar_position:1},o={unversionedId:"tutorial-extras/how-doodler-works",id:"tutorial-extras/how-doodler-works",isDocsHomePage:!1,title:"[ADVANCED] How Doodler works",description:"(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation)",source:"@site/docs/tutorial-extras/how-doodler-works.md",sourceDirName:"tutorial-extras",slug:"/tutorial-extras/how-doodler-works",permalink:"/dash_doodler/docs/tutorial-extras/how-doodler-works",editUrl:"https://github.com/dbuscombe-usgs/dash_doodler/edit/master/website/docs/tutorial-extras/how-doodler-works.md",version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Getting help",permalink:"/dash_doodler/docs/tutorial-basics/getting-help"},next:{title:"[ADVANCED] Serving Doodler as a web application for others to use",permalink:"/dash_doodler/docs/tutorial-extras/deploy-server"}},l=[{value:"Overview",id:"overview",children:[]},{value:"Sparse annotation or &#39;doodling&#39;",id:"sparse-annotation-or-doodling",children:[]},{value:"Image standardization",id:"image-standardization",children:[]},{value:"Feature extraction",id:"feature-extraction",children:[]},{value:"Initial Pixel Classifier",id:"initial-pixel-classifier",children:[]},{value:"Spatial filtering of MLP predictions",id:"spatial-filtering-of-mlp-predictions",children:[]},{value:"Conditional Random Field Modeling",id:"conditional-random-field-modeling",children:[]},{value:"Spatial filtering of CRF prediction",id:"spatial-filtering-of-crf-prediction",children:[]}],d={toc:l};function c(e){var t=e.components,s=Object(r.a)(e,["components"]);return Object(n.b)("wrapper",Object(i.a)({},d,s,{components:t,mdxType:"MDXLayout"}),Object(n.b)("p",null,"(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation)"),Object(n.b)("p",null,"Images are labeled in sessions. During a session, a Machine Learning model is built progressively using provided labels from each image. Below, we illustrate each of the various processing steps in turn, using a single example of a 2-class labeling exercise. The two classes are 'land' (red) and 'water' (blue)."),Object(n.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(n.b)("div",{parentName:"div",className:"admonition-heading"},Object(n.b)("h5",{parentName:"div"},Object(n.b)("span",{parentName:"h5",className:"admonition-icon"},Object(n.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(n.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(n.b)("div",{parentName:"div",className:"admonition-content"},Object(n.b)("p",{parentName:"div"},"Doodler 'learns as you go'. In a Doodler session, the skill of predictions usually improves the more images you doodle in a single session. That's because it updates the model with each set if new image feature-class pairings you provide it"))),Object(n.b)("h3",{id:"overview"},"Overview"),Object(n.b)("p",null,"The figure below depicts a typical Doodler session in which images (from left to right) are labeled sequentially. The figure is read from left to right, depicting the sequential nature of image labeling, as well as from top to bottom, which depicts the sequence of processes that occur to collectively turn the image at the top, to the label image at the bottom."),Object(n.b)("p",null,Object(n.b)("img",{src:a(178).default})),Object(n.b)("p",null,"There is a lot going on in this figure, and it is read from top to bottom (single image) and from left to right (a sequence of images in a session), so let's break it down a little ..."),Object(n.b)("h3",{id:"sparse-annotation-or-doodling"},"Sparse annotation or 'doodling'"),Object(n.b)("p",null,"It all begins with your inputs - doodles. This is what the subsequent list of operations are entirely based upon, together with some spatial logic and some assumptions regarding the types of 'features' to extract from imagery that would collectively best predict the classes."),Object(n.b)("p",null,Object(n.b)("img",{src:a(179).default})),Object(n.b)("h3",{id:"image-standardization"},"Image standardization"),Object(n.b)("p",null,"Doodler first standardizes the images, which means every pixel value is scaled to have a mean of zero and a variance of 1, or 'unit' variance. For the model, this ensures every image has a similar data distribution. It also acts like a 'white balance' filter for the image, enhancing color and contrast."),Object(n.b)("p",null,Object(n.b)("img",{src:a(180).default})),Object(n.b)("h3",{id:"feature-extraction"},"Feature extraction"),Object(n.b)("p",null,"Doodler estimates a dense (i.e. per-pixel) label image from an input image and your sparse annotations or 'doodles'. It does this by first extracting image features in a prescribed way (i.e. the image features are extracted in the same way each time) and matching those features to classes using Machine Learning."),Object(n.b)("p",null,"Doodler extracts a series of 2D feature maps from the standardized input imagery. From each sample image, 75 2D image feature maps are extracted (5 feature types, across 15 unique spatial scales)"),Object(n.b)("p",null,"The 5 extracted feature types are (from left to right in the image below):"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"Relative location: the distance in pixels of each pixel to the image origin"),Object(n.b)("li",{parentName:"ul"},"Intensity: Gaussian blur over a range of scales"),Object(n.b)("li",{parentName:"ul"},"Edges: Sobel filter of the Gaussian blurred images"),Object(n.b)("li",{parentName:"ul"},"Primary texture: Matrix of texture values extacted over a range of scales as the 1st eigenvalue of the Hessian Matrix"),Object(n.b)("li",{parentName:"ul"},"Secondary texture: Matrix of texture values extacted over a range of scales as the 2nd eigenvalue of the Hessian Matrix")),Object(n.b)("p",null,"In the figure below, only the 5 feature maps extracted at the smallest and largest scales are shown for brevity:"),Object(n.b)("p",null,Object(n.b)("img",{src:a(181).default})),Object(n.b)("h3",{id:"initial-pixel-classifier"},"Initial Pixel Classifier"),Object(n.b)("p",null,"As stated above, Doodler extracts features from imagery, and pairs those extracted features with their class distinctions provided by you in the form of 'doodles'. How that pairing occurs is achieved using Machine Learning, or 'ML' for short. Doodler uses two particular types of ML. The first is called a 'Multilayer Perceptron', or MLP for short. The seond ML model we use is called a \"CRF' and we'll talk about that later."),Object(n.b)("p",null,'The first image is "doodled", and the program creates a MLP model that predicts the class of each pixel according to the distribution of features extracted from the vicinity of that pixel.'),Object(n.b)("p",null,"Those 2D features are then flattened to a 1D array of length M, and stack them N deep, where N is the number of individual 2D feature maps, such that the resulting feature stack is size MxN. Provided label annotations are provided at a subset, i, of the M locations, M_i, so the training data is the subset {MxN}_i. That training data is further subsampled by a user-defined factor, then used to train a MLP classifier."),Object(n.b)("p",null,"Below is a graphic showing, for this particular sample image we are using in this example, how the trained MLP model is making decisions based on pairs of input features. Each of the 9 subplots shown depict a 'decision surface' for the two classes (water in blue and land in red) based on a pair of features. The colored markers show actual feature values extracted from image-feature-class pairings. As you can see, the MLP model can extrapolate a decision surface beyond the extents of the data, which is useful in situations when data is encountered with relatively unusual feature values."),Object(n.b)("p",null,Object(n.b)("img",{src:a(182).default})),Object(n.b)("p",null,"In reality, the MLP model does this on all 75 features and their respective combinations (2775 unique pairs of 75 features) simultaneously. It combines this information to predict a unique class (encoded as an integer value) for each pixel. The computations happen in 1D, i.e. on arrays of length MxN, which are then reshaped. Therefore the only spatial information used in prediction is that of the relative location feature maps."),Object(n.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(n.b)("div",{parentName:"div",className:"admonition-heading"},Object(n.b)("h5",{parentName:"div"},Object(n.b)("span",{parentName:"h5",className:"admonition-icon"},Object(n.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(n.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(n.b)("div",{parentName:"div",className:"admonition-content"},Object(n.b)("p",{parentName:"div"},"The program adopts a strategy similar to that described by ",Object(n.b)("a",{parentName:"p",href:"https://www.mdpi.com/2076-3263/8/7/244"},"Buscombe and Ritchie (2018)"),", in that a 'global' model trained on many samples is used to provide an initial segmentation on each sample image, then that initial segmentation is refined by a CRF, which operates on a task specific level. In ",Object(n.b)("a",{parentName:"p",href:"references"},"Buscombe and Ritchie (2018)"),", the model was a deep neural network trained in advance on large numbers of samples and labels. Here, the model is built as we go, building progressively from user inputs. Doodler uses a MLP as the baseline global model, and the CRF implementation is the same as that desribed by Buscombe and Ritchie (2018)"))),Object(n.b)("h3",{id:"spatial-filtering-of-mlp-predictions"},"Spatial filtering of MLP predictions"),Object(n.b)("p",null,"Each MLP prediction (a label matrix of integer values, each integer corresponding to a unique class). That corresponds to the left image in the figure below. You can see there is a lot of noise in the prediction; for example, and most notably, there are several small 'islands' of land in the water that are model errors. Therefore each label image is filtered using two complementary procedures that operate in the spatial domain."),Object(n.b)("p",null,Object(n.b)("img",{src:a(183).default})),Object(n.b)("p",null,"The first filtering exercise (the outputs of which are labeled \"b) Filtered\" in the example figure above) operates on the one-hot encoded stack of labels. For each, pixel 'islands' less than a certain size are removed (filled in with the value of the surrounding area). Additionally, pixel 'holes' are also sealed (filled in with the value of the surrounding area). You can see in the example, by comparing a) and b) in the above figure, that many islands were removed in this process on this particular example."),Object(n.b)("p",null,"The second filter then determines a 'null class' based on those pixels that are furthest away from similar classes. Those pixels occur at the transition areas between large contiguous regions of same-class. The process by which this is acheived is described in the figure below:"),Object(n.b)("p",null,Object(n.b)("img",{src:a(184).default})),Object(n.b)("p",null,"The intuition for 'zeroing' these pixels is to allow a further model, described below, to estimate the appropriate class values for pixels in those transition areas."),Object(n.b)("h3",{id:"conditional-random-field-modeling"},"Conditional Random Field Modeling"),Object(n.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(n.b)("div",{parentName:"div",className:"admonition-heading"},Object(n.b)("h5",{parentName:"div"},Object(n.b)("span",{parentName:"h5",className:"admonition-icon"},Object(n.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(n.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(n.b)("div",{parentName:"div",className:"admonition-content"},Object(n.b)("p",{parentName:"div"},"Already you can see how we are building a lot of robustness to natural variability:"),Object(n.b)("ol",{parentName:"div"},Object(n.b)("li",{parentName:"ol"},"images are standardized"),Object(n.b)("li",{parentName:"ol"},"Other measures are made to prevent model overfitting, such as downsampling"),Object(n.b)("li",{parentName:"ol"},"Different types of image and location features are used and extracted at a variety of scales"),Object(n.b)("li",{parentName:"ol"},"MLP outputs are filtered using relative spatial information")),Object(n.b)("p",{parentName:"div"},"Next we'll go even further by making use of both global and local predictions."),Object(n.b)("p",{parentName:"div"},"The global predictions are provided by the MLP model. They are called 'global' because the model is built from all doodled images in a sequence."),Object(n.b)("p",{parentName:"div"},"The local predictions are provided by a different type of ML model, called a CRF, which is explained below."))),Object(n.b)("p",null,"That initial MLP provides an initial estimate of the entire label estimate, which is fed (with the original image) to a secondary post-processing model based on a fully connected Conditional Random Field model, or CRF for short. The CRF model refines the label image, using the MLP model output as priors that are refined to posteriors given the specific image. As such, the MLP model is treated as a initial model and the CRF model is for 'local' (i.e. image-specific) refinement."),Object(n.b)("p",null,"The CRF builds a model for the likelihood of the MLP-predicted labels based on the distributions of features it extracts from the imagery, and can reclassify pixels (it is intended to do so). Its feature extraction and decision making behavior is complex and governed by parameters. That prediction then is further refined by applying the model to numerous transformed versions of the image, making predictions, untransforming, and then averaging the stack of resulting predictions. This concept is called test-time-augmentation and is illustrated in the figure below as outputs from using 10 'test-time augmented' inputs:"),Object(n.b)("p",null,Object(n.b)("img",{src:a(185).default})),Object(n.b)("h3",{id:"spatial-filtering-of-crf-prediction"},"Spatial filtering of CRF prediction"),Object(n.b)("p",null,"The label image that is the result of the above process is yet further filtered using the same two-part spatial procedure described above for the MLP model outputs. Usually, these procedures revert fewer pixel class values than the equivalent prior process on the MLP model outputs. The outputs are shown in 'b) and c)' in the figure below. A final additional step not used on MLP model outputs is to 'inpaint' the pixels in identified transition areas using nearest neighbor interpolation ('d)' in the figure below)"),Object(n.b)("p",null,Object(n.b)("img",{src:a(186).default})),Object(n.b)("p",null,"A final label image is then stored in disk:"),Object(n.b)("p",null,Object(n.b)("img",{src:a(187).default})))}c.isMDXComponent=!0}}]);