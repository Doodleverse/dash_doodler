<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.73">
<link rel="alternate" type="application/rss+xml" href="/dash_doodler/blog/rss.xml" title="Doodler Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/dash_doodler/blog/atom.xml" title="Doodler Blog Atom Feed"><title data-react-helmet="true">[ADVANCED] How Doodler works | Doodler</title><meta data-react-helmet="true" property="og:url" content="https://dbuscombe-usgs.github.io/dash_doodler/docs/tutorial-extras/how-doodler-works"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="[ADVANCED] How Doodler works | Doodler"><meta data-react-helmet="true" name="description" content="(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation)"><meta data-react-helmet="true" property="og:description" content="(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation)"><link data-react-helmet="true" rel="shortcut icon" href="/dash_doodler/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://dbuscombe-usgs.github.io/dash_doodler/docs/tutorial-extras/how-doodler-works"><link data-react-helmet="true" rel="alternate" href="https://dbuscombe-usgs.github.io/dash_doodler/docs/tutorial-extras/how-doodler-works" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://dbuscombe-usgs.github.io/dash_doodler/docs/tutorial-extras/how-doodler-works" hreflang="x-default"><link rel="stylesheet" href="/dash_doodler/assets/css/styles.df7ce140.css">
<link rel="preload" href="/dash_doodler/assets/js/styles.80f7ac2f.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/runtime~main.67ae175f.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/main.35537849.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/1.cff2a302.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/2.7b62eef7.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/40.8d438150.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/42.91e641e5.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/935f2afb.6b4e7bd2.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/17896441.65b499a8.js" as="script">
<link rel="preload" href="/dash_doodler/assets/js/174ec309.34f1e24c.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle" type="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/dash_doodler/"><img src="/dash_doodler/img/doodler-logo.png" alt="My Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/dash_doodler/img/doodler-logo.png" alt="My Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title"></strong></a><a class="navbar__item navbar__link navbar__link--active" href="/dash_doodler/docs/intro">Tutorials</a><a class="navbar__item navbar__link" href="/dash_doodler/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/dbuscombe-usgs/dash_doodler" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Doodler github page</a><div class="react-toggle displayOnlyInLargeViewport_GrZ2 react-toggle--disabled" role="button" tabindex="-1"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_71bT">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/dash_doodler/"><img src="/dash_doodler/img/doodler-logo.png" alt="My Site Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/dash_doodler/img/doodler-logo.png" alt="My Site Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link navbar__link--active" href="/dash_doodler/docs/intro">Tutorials</a></li><li class="menu__list-item"><a class="menu__link" href="/dash_doodler/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/dbuscombe-usgs/dash_doodler" target="_blank" rel="noopener noreferrer" class="menu__link">Doodler github page</a></li></ul></div></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/dash_doodler/docs/intro">What is Doodler for?</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Doodler - Basics</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/tutorial-basics/deploy-local">Installing Doodler on a PC for your own use</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/tutorial-basics/what-to-do">What to do</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/tutorial-basics/how-to-doodle">How to Doodle well</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/tutorial-basics/faqs">Frequently Asked Questions</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/tutorial-basics/getting-help">Getting help</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Doodler - Extras</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/dash_doodler/docs/tutorial-extras/how-doodler-works">[ADVANCED] How Doodler works</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/dash_doodler/docs/tutorial-extras/deploy-server">[ADVANCED] Serving Doodler as a web application for others to use</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/dash_doodler/docs/tutorial-extras/how-to-contribute">[ADVANCED] How to contribute</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/dash_doodler/docs/tutorial-extras/next-steps">[ADVANCED] Next steps .... training a Deep Learning model for image segmentation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/dash_doodler/docs/tutorial-extras/references">[ADVANCED] References</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Case Studies</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/case-studies/case-study1">Case Study 1: Greyscale geophysical imagery</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/case-studies/case-study2">Case Study 3: NOAA post-hurricane reconnaissance aerial imagery</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/dash_doodler/docs/case-studies/case-study3">Case Study 3: Sentinel-2 shoreline time-series imagery</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">[ADVANCED] How Doodler works</h1></header><div class="markdown"><p>(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation)</p><p>Images are labeled in sessions. During a session, a Machine Learning model is built progressively using provided labels from each image. Below, we illustrate each of the various processing steps in turn, using a single example of a 2-class labeling exercise. The two classes are &#x27;land&#x27; (red) and &#x27;water&#x27; (blue).</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tip</h5></div><div class="admonition-content"><p>Doodler &#x27;learns as you go&#x27;. In a Doodler session, the skill of predictions usually improves the more images you doodle in a single session. That&#x27;s because it updates the model with each set if new image feature-class pairings you provide it</p></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="overview"></a>Overview<a class="hash-link" href="#overview" title="Direct link to heading">#</a></h3><p>The figure below depicts a typical Doodler session in which images (from left to right) are labeled sequentially. The figure is read from left to right, depicting the sequential nature of image labeling, as well as from top to bottom, which depicts the sequence of processes that occur to collectively turn the image at the top, to the label image at the bottom.</p><p><img src="/dash_doodler/assets/images/paperfig_RFchain_ann-b56a8371bc87b77b219a9e0ca2226667.jpg"></p><p>There is a lot going on in this figure, and it is read from top to bottom (single image) and from left to right (a sequence of images in a session), so let&#x27;s break it down a little ...</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sparse-annotation-or-doodling"></a>Sparse annotation or &#x27;doodling&#x27;<a class="hash-link" href="#sparse-annotation-or-doodling" title="Direct link to heading">#</a></h3><p>It all begins with your inputs - doodles. This is what the subsequent list of operations are entirely based upon, together with some spatial logic and some assumptions regarding the types of &#x27;features&#x27; to extract from imagery that would collectively best predict the classes.</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_image_doodles_labelgen-e548fceb28a327b50a42c49e1d3af736.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="image-standardization"></a>Image standardization<a class="hash-link" href="#image-standardization" title="Direct link to heading">#</a></h3><p>Doodler first standardizes the images, which means every pixel value is scaled to have a mean of zero and a variance of 1, or &#x27;unit&#x27; variance. For the model, this ensures every image has a similar data distribution. It also acts like a &#x27;white balance&#x27; filter for the image, enhancing color and contrast.</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_image_filt_labelgen-505bc32ee695274f105461ba311a8235.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="feature-extraction"></a>Feature extraction<a class="hash-link" href="#feature-extraction" title="Direct link to heading">#</a></h3><p>Doodler estimates a dense (i.e. per-pixel) label image from an input image and your sparse annotations or &#x27;doodles&#x27;. It does this by first extracting image features in a prescribed way (i.e. the image features are extracted in the same way each time) and matching those features to classes using Machine Learning.</p><p>Doodler extracts a series of 2D feature maps from the standardized input imagery. From each sample image, 75 2D image feature maps are extracted (5 feature types, across 15 unique spatial scales)</p><p>The 5 extracted feature types are (from left to right in the image below):</p><ul><li>Relative location: the distance in pixels of each pixel to the image origin</li><li>Intensity: Gaussian blur over a range of scales</li><li>Edges: Sobel filter of the Gaussian blurred images</li><li>Primary texture: Matrix of texture values extacted over a range of scales as the 1st eigenvalue of the Hessian Matrix</li><li>Secondary texture: Matrix of texture values extacted over a range of scales as the 2nd eigenvalue of the Hessian Matrix</li></ul><p>In the figure below, only the 5 feature maps extracted at the smallest and largest scales are shown for brevity:</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_image_feats_labelgen-1fdb12ad2e7666a13412a34463fa5fc9.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="initial-pixel-classifier"></a>Initial Pixel Classifier<a class="hash-link" href="#initial-pixel-classifier" title="Direct link to heading">#</a></h3><p>As stated above, Doodler extracts features from imagery, and pairs those extracted features with their class distinctions provided by you in the form of &#x27;doodles&#x27;. How that pairing occurs is achieved using Machine Learning, or &#x27;ML&#x27; for short. Doodler uses two particular types of ML. The first is called a &#x27;Multilayer Perceptron&#x27;, or MLP for short. The seond ML model we use is called a &quot;CRF&#x27; and we&#x27;ll talk about that later.</p><p>The first image is &quot;doodled&quot;, and the program creates a MLP model that predicts the class of each pixel according to the distribution of features extracted from the vicinity of that pixel.</p><p>Those 2D features are then flattened to a 1D array of length M, and stack them N deep, where N is the number of individual 2D feature maps, such that the resulting feature stack is size MxN. Provided label annotations are provided at a subset, i, of the M locations, M_i, so the training data is the subset {MxN}_i. That training data is further subsampled by a user-defined factor, then used to train a MLP classifier.</p><p>Below is a graphic showing, for this particular sample image we are using in this example, how the trained MLP model is making decisions based on pairs of input features. Each of the 9 subplots shown depict a &#x27;decision surface&#x27; for the two classes (water in blue and land in red) based on a pair of features. The colored markers show actual feature values extracted from image-feature-class pairings. As you can see, the MLP model can extrapolate a decision surface beyond the extents of the data, which is useful in situations when data is encountered with relatively unusual feature values.</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_RFdecsurf_labelgen_ann-c6ae8f7b89f946437b6f998740e9a2b7.png"></p><p>In reality, the MLP model does this on all 75 features and their respective combinations (2775 unique pairs of 75 features) simultaneously. It combines this information to predict a unique class (encoded as an integer value) for each pixel. The computations happen in 1D, i.e. on arrays of length MxN, which are then reshaped. Therefore the only spatial information used in prediction is that of the relative location feature maps.</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tip</h5></div><div class="admonition-content"><p>The program adopts a strategy similar to that described by <a href="https://www.mdpi.com/2076-3263/8/7/244" target="_blank" rel="noopener noreferrer">Buscombe and Ritchie (2018)</a>, in that a &#x27;global&#x27; model trained on many samples is used to provide an initial segmentation on each sample image, then that initial segmentation is refined by a CRF, which operates on a task specific level. In <a href="/dash_doodler/docs/tutorial-extras/references">Buscombe and Ritchie (2018)</a>, the model was a deep neural network trained in advance on large numbers of samples and labels. Here, the model is built as we go, building progressively from user inputs. Doodler uses a MLP as the baseline global model, and the CRF implementation is the same as that desribed by Buscombe and Ritchie (2018)</p></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spatial-filtering-of-mlp-predictions"></a>Spatial filtering of MLP predictions<a class="hash-link" href="#spatial-filtering-of-mlp-predictions" title="Direct link to heading">#</a></h3><p>Each MLP prediction (a label matrix of integer values, each integer corresponding to a unique class). That corresponds to the left image in the figure below. You can see there is a lot of noise in the prediction; for example, and most notably, there are several small &#x27;islands&#x27; of land in the water that are model errors. Therefore each label image is filtered using two complementary procedures that operate in the spatial domain.</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_rf_label_filtered_labelgen-4a390b5e15fdb1eda7e8e7070960f6bc.png"></p><p>The first filtering exercise (the outputs of which are labeled &quot;b) Filtered&quot; in the example figure above) operates on the one-hot encoded stack of labels. For each, pixel &#x27;islands&#x27; less than a certain size are removed (filled in with the value of the surrounding area). Additionally, pixel &#x27;holes&#x27; are also sealed (filled in with the value of the surrounding area). You can see in the example, by comparing a) and b) in the above figure, that many islands were removed in this process on this particular example.</p><p>The second filter then determines a &#x27;null class&#x27; based on those pixels that are furthest away from similar classes. Those pixels occur at the transition areas between large contiguous regions of same-class. The process by which this is acheived is described in the figure below:</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_rf_spatfilt_dist_labelgen_ann-d343c8e126bbdc7114ddbf63d199f0c0.png"></p><p>The intuition for &#x27;zeroing&#x27; these pixels is to allow a further model, described below, to estimate the appropriate class values for pixels in those transition areas.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="conditional-random-field-modeling"></a>Conditional Random Field Modeling<a class="hash-link" href="#conditional-random-field-modeling" title="Direct link to heading">#</a></h3><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Tip</h5></div><div class="admonition-content"><p>Already you can see how we are building a lot of robustness to natural variability:</p><ol><li>images are standardized</li><li>Other measures are made to prevent model overfitting, such as downsampling</li><li>Different types of image and location features are used and extracted at a variety of scales</li><li>MLP outputs are filtered using relative spatial information</li></ol><p>Next we&#x27;ll go even further by making use of both global and local predictions.</p><p>The global predictions are provided by the MLP model. They are called &#x27;global&#x27; because the model is built from all doodled images in a sequence.</p><p>The local predictions are provided by a different type of ML model, called a CRF, which is explained below.</p></div></div><p>That initial MLP provides an initial estimate of the entire label estimate, which is fed (with the original image) to a secondary post-processing model based on a fully connected Conditional Random Field model, or CRF for short. The CRF model refines the label image, using the MLP model output as priors that are refined to posteriors given the specific image. As such, the MLP model is treated as a initial model and the CRF model is for &#x27;local&#x27; (i.e. image-specific) refinement.</p><p>The CRF builds a model for the likelihood of the MLP-predicted labels based on the distributions of features it extracts from the imagery, and can reclassify pixels (it is intended to do so). Its feature extraction and decision making behavior is complex and governed by parameters. That prediction then is further refined by applying the model to numerous transformed versions of the image, making predictions, untransforming, and then averaging the stack of resulting predictions. This concept is called test-time-augmentation and is illustrated in the figure below as outputs from using 10 &#x27;test-time augmented&#x27; inputs:</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_crf_tta_labelgen-4cb80119f53fcfac89105351cbfadbdd.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="spatial-filtering-of-crf-prediction"></a>Spatial filtering of CRF prediction<a class="hash-link" href="#spatial-filtering-of-crf-prediction" title="Direct link to heading">#</a></h3><p>The label image that is the result of the above process is yet further filtered using the same two-part spatial procedure described above for the MLP model outputs. Usually, these procedures revert fewer pixel class values than the equivalent prior process on the MLP model outputs. The outputs are shown in &#x27;b) and c)&#x27; in the figure below. A final additional step not used on MLP model outputs is to &#x27;inpaint&#x27; the pixels in identified transition areas using nearest neighbor interpolation (&#x27;d)&#x27; in the figure below)</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_crf_label_filtered_labelgen-ee93ef47d939db64179db2323637f279.png"></p><p>A final label image is then stored in disk:</p><p><img src="/dash_doodler/assets/images/D800_20160308_222129lr03-3_db_image_label_final_labelgen-9a4b346d54d7a617c37c1e993fd94dff.png"></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/dbuscombe-usgs/dash_doodler/edit/master/website/docs/tutorial-extras/how-doodler-works.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-label="Edit page"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/dash_doodler/docs/tutorial-basics/getting-help"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Getting help</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/dash_doodler/docs/tutorial-extras/deploy-server"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[ADVANCED] Serving Doodler as a web application for others to use Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link">Overview</a></li><li><a href="#sparse-annotation-or-doodling" class="table-of-contents__link">Sparse annotation or &#39;doodling&#39;</a></li><li><a href="#image-standardization" class="table-of-contents__link">Image standardization</a></li><li><a href="#feature-extraction" class="table-of-contents__link">Feature extraction</a></li><li><a href="#initial-pixel-classifier" class="table-of-contents__link">Initial Pixel Classifier</a></li><li><a href="#spatial-filtering-of-mlp-predictions" class="table-of-contents__link">Spatial filtering of MLP predictions</a></li><li><a href="#conditional-random-field-modeling" class="table-of-contents__link">Conditional Random Field Modeling</a></li><li><a href="#spatial-filtering-of-crf-prediction" class="table-of-contents__link">Spatial filtering of CRF prediction</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/dash_doodler/docs/intro">Tutorials</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/dbuscombe-usgs/coast_train" target="_blank" rel="noopener noreferrer" class="footer__link-item">Coast Train</a></li><li class="footer__item"><a href="https://www.usgs.gov/centers/pcmsc/science/remote-sensing-coastal-change?qt-science_center_objects=0#qt-science_center_objects" target="_blank" rel="noopener noreferrer" class="footer__link-item">USGS Remote Sensing Coastal Change</a></li><li class="footer__item"><a href="https://coastalimagelabeler.science/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Coastal Image Labeler by Dr Evan Goldstein</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/dash_doodler/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/dbuscombe-usgs/dash_doodler" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github</a></li><li class="footer__item"><a href="https://www.makesense.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Makesense.ai (an alternative way to segment imagery)</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Doodler is written and maintained by Daniel Buscombe, Marda Science, LLC, contracted to the U.S. Geological Survey Pacific Coastal and Marine Science Center in Santa Cruz, CA. Doodler development is funded by the U.S. Geological Survey Coastal Hazards Program, and is for the primary usage of U.S. Geological Survey scientists, researchers and affiliated colleagues working on the Hurricane Florence Supplemental Project and other coastal hazards research. Thanks to Jon Warrick, Phil Wernette, Chris Sherwood, Jenna Brown, Andy Ritchie, Jin-Si Over, Christine Kranenburg, and the rest of the Florence Supplemental team; to Evan Goldstein and colleagues at University of North Carolina Greensboro; Leslie Hsu at the USGS Community for Data Integration; and LCDR Brodie Wells, formerly of Naval Postgraduate School, Monterey. Copyright Â© 2021 Marda Science, LLC. </div></div></div></footer></div>
<script src="/dash_doodler/assets/js/styles.80f7ac2f.js"></script>
<script src="/dash_doodler/assets/js/runtime~main.67ae175f.js"></script>
<script src="/dash_doodler/assets/js/main.35537849.js"></script>
<script src="/dash_doodler/assets/js/1.cff2a302.js"></script>
<script src="/dash_doodler/assets/js/2.7b62eef7.js"></script>
<script src="/dash_doodler/assets/js/40.8d438150.js"></script>
<script src="/dash_doodler/assets/js/42.91e641e5.js"></script>
<script src="/dash_doodler/assets/js/935f2afb.6b4e7bd2.js"></script>
<script src="/dash_doodler/assets/js/17896441.65b499a8.js"></script>
<script src="/dash_doodler/assets/js/174ec309.34f1e24c.js"></script>
</body>
</html>